# Local AI & High-Performance Computing Lab

Este reposit√≥rio documenta a implementa√ß√£o de um ambiente local para execu√ß√£o de LLMs (Large Language Models) e gera√ß√£o de imagens via Stable Diffusion, utilizando acelera√ß√£o de hardware (GPU).

## üõ†Ô∏è Infraestrutura e Configura√ß√£o

### Sistema Operacional: Windows + WSL 2
Optei por utilizar o **WSL 2 (Windows Subsystem for Linux)** para criar um ambiente de desenvolvimento h√≠brido.

* **Por que WSL 2?**
    * Permite o uso de ferramentas nativas Linux (essenciais para Data Science e IA) sem a sobrecarga de uma M√°quina Virtual tradicional.
    * **GPU Passthrough:** O WSL 2 oferece acesso direto aos drivers da NVIDIA (CUDA) instalados no Windows, permitindo que o container Linux utilize todo o poder da minha RTX 5060 para c√°lculos tensoriais.
    * Facilita a integra√ß√£o com Docker, eliminando problemas de compatibilidade comuns no Windows nativo.

**Specs do Ambiente:**
* **CPU:** Intel Core i5 12400F
* **GPU:** NVIDIA RTX 5060 (8GB VRAM)
* **RAM:** 32GB
* **OS:** Ubuntu 22.04 LTS rodando sobre Windows 11 via WSL 2.

### ‚öôÔ∏è Configura√ß√£o do Ambiente Linux (Ubuntu)

Ap√≥s a ativa√ß√£o do subsistema, foi realizada a configura√ß√£o inicial do ambiente Linux:

1.  **Distribui√ß√£o:** Ubuntu 22.04 LTS (Jammy Jellyfish).
    * *Escolha:* Padr√£o da ind√∫stria para servidores e desenvolvimento de IA, garantindo compatibilidade m√°xima com bibliotecas como PyTorch e TensorFlow.
2.  **Gest√£o de Usu√°rios e Permiss√µes:**
    * Cria√ß√£o de usu√°rio n√£o-root dedicado para desenvolvimento.
    * Configura√ß√£o de privil√©gios de superusu√°rio (`sudo`) para administra√ß√£o de pacotes e servi√ßos.
3.  **Arquitetura de Filesystem:**
    * O ambiente Linux opera em seu pr√≥prio sistema de arquivos virtual (VHDX), mas mant√©m acesso de leitura/escrita aos arquivos do host Windows (montados em `/mnt/c/`), facilitando a troca de dados entre os sistemas.

    ### üöÄ Acelera√ß√£o de Hardware e Container Runtime

Para orquestrar os modelos de IA, configurei um ambiente baseado em containers Docker com suporte a GPGPU (General-Purpose computing on Graphics Processing Units).

1.  **Integra√ß√£o CUDA (NVIDIA Driver Passthrough):**
    * Verifica√ß√£o realizada via `nvidia-smi`.
    * O WSL 2 abstrai o hardware, permitindo que bibliotecas como PyTorch acessem a RTX 5060 diretamente atrav√©s da camada de compatibilidade do DirectX 12/WDDM 2.9, sem necessidade de drivers propriet√°rios no kernel Linux.

2.  **Docker Engine (Native Linux):**
    * Optei pela instala√ß√£o da *Docker Engine* nativa no ambiente Ubuntu (em vez do Docker Desktop for Windows).
    * **Benef√≠cio de Performance:** Redu√ß√£o de overhead de I/O de sistema de arquivos e gerenciamento de rede direto pelo kernel Linux.
    * Configura√ß√£o de grupos de usu√°rios (`usermod -aG docker`) para execu√ß√£o segura de containers sem privil√©gios de root (rootless execution context).

    ## üß† Engine de Infer√™ncia e Runtime

Para habilitar a execu√ß√£o de modelos de Deep Learning dentro de containers, foi necess√°ria a configura√ß√£o do runtime propriet√°rio da NVIDIA.

### 1. NVIDIA Container Toolkit
O Docker padr√£o isola o hardware do host. Para "perfurar" esse isolamento de forma segura, instalei o `nvidia-container-toolkit`.
* **Fun√ß√£o:** Atua como um *wrapper* para o runtime `runc` do Docker, injetando automaticamente os drivers e bibliotecas CUDA (libcuda.so, libcudart.so) dentro do container na hora da execu√ß√£o.
* **Comando de valida√ß√£o:** `sudo nvidia-ctk runtime configure --runtime=docker`

### 2. Ollama (LLM Backend)
Utilizei o **Ollama** como servidor de infer√™ncia local.
* **Modelo de Teste:** Llama 3.2 (Quantiza√ß√£o 4-bit).
* **Performance:** A execu√ß√£o ocorre nativamente no Linux, acessando a GPU via chamadas CUDA diretas, resultando em lat√™ncia m√≠nima para gera√ß√£o de tokens.

## üîß Troubleshooting e Li√ß√µes Aprendidas

Durante a configura√ß√£o do servi√ßo de infer√™ncia, encontrei e solucionei os seguintes comportamentos:

1.  **Conflito de Portas (Error: address already in use):**
    * **Causa:** Tentativa de iniciar o servidor `ollama serve` manualmente enquanto o servi√ßo de background (systemd) j√° estava ativo p√≥s-instala√ß√£o.
    * **Solu√ß√£o:** O Ollama opera como um daemon. A intera√ß√£o deve ser feita diretamente via comandos de cliente (`ollama run`), sem necessidade de invocar o servidor manualmente na sess√£o do usu√°rio.

2.  **Sintaxe de Modelos:**
    * A CLI do Ollama requer a nomenclatura exata dos modelos conforme o registro oficial (ex: `llama3.2` em vez de `llama 3.2`), sem espa√ßos que quebrem o parsing do manifesto.

    ## üñ•Ô∏è User Interface & Orchestration (Open WebUI)

Para proporcionar uma experi√™ncia de uso compar√°vel a solu√ß√µes comerciais (SaaS), implementei uma interface web moderna via container Docker.

### Arquitetura da Solu√ß√£o:
* **Frontend/Application Server:** Open WebUI rodando em container Docker isolado.
* **Comunica√ß√£o Inter-Processos:** Utiliza√ß√£o da flag `--add-host=host.docker.internal:host-gateway` para permitir que o container Docker acesse o servi√ßo do Ollama rodando no host (WSL 2), superando o isolamento padr√£o de rede do Docker.
* **Persist√™ncia de Dados:** Configura√ß√£o de volumes Docker (`-v`) para garantir a durabilidade do hist√≥rico de conversas e configura√ß√µes de usu√°rio (RAG documents) entre reinicializa√ß√µes.

### Stack Visual:
A interface permite:
1.  Troca din√¢mica de modelos (Hot-swapping).
2.  Formata√ß√£o de c√≥digo com Syntax Highlighting (essencial para Code Review).
3.  Hist√≥rico local e privado.

### ‚è±Ô∏è Lat√™ncia de Inicializa√ß√£o (Cold Start)
Notei que, ao iniciar o container do Open WebUI pela primeira vez, existe um *delay* de inicializa√ß√£o da aplica√ß√£o interna (backend Python).
* **Sintoma:** O navegador retorna `ERR_EMPTY_RESPONSE` nos primeiros 30-60 segundos.
* **Diagn√≥stico:** Verifica√ß√£o via `docker logs -f open-webui` confirmou que o processo de *boot* do servidor Uvicorn ainda estava em andamento.
* **Solu√ß√£o:** Aguardar a mensagem `Application startup complete` nos logs antes de tentar o acesso via browser.

### üåê Configura√ß√£o de Rede e Exposi√ß√£o de Servi√ßos

Para viabilizar a comunica√ß√£o entre o container da aplica√ß√£o (Open WebUI) e o servi√ßo de infer√™ncia no host (Ollama), realizei duas configura√ß√µes cr√≠ticas de rede:

1.  **Host Binding (0.0.0.0):**
    * **Problema:** O Ollama, por padr√£o, escuta apenas na interface de loopback (`127.0.0.1`), rejeitando conex√µes da bridge network do Docker.
    * **Solu√ß√£o:** Altera√ß√£o via `systemd` override para definir a vari√°vel de ambiente `OLLAMA_HOST=0.0.0.0`, permitindo escuta em todas as interfaces de rede.

2.  **DNS Interno do Docker:**
    * Configurei o frontend para apontar para `http://host.docker.internal:11434`.
    * Isso utiliza o resolver interno do Docker para encaminhar as requisi√ß√µes HTTP da API para o gateway do host (WSL 2), transpassando o isolamento do container.

    ## ü§ñ Sele√ß√£o de Modelos (Model Zoo)

Para otimizar o uso dos recursos da GPU (8GB VRAM), selecionei uma arquitetura de modelos especializados em vez de um √∫nico modelo monol√≠tico:

1.  **General Purpose (Racioc√≠nio e Chat):** `Meta Llama 3.1 8B` (Quantiza√ß√£o 4-bit).
    * Escolhido pelo equil√≠brio entre coer√™ncia l√≥gica e velocidade de infer√™ncia.
2.  **Development & Coding:** `DeepSeek Coder V2`.
    * Modelo especializado (Fine-tuned) em linguagens de programa√ß√£o (C, Java, Python), oferecendo performance superior em gera√ß√£o de sintaxe e refatora√ß√£o de c√≥digo.
3.  **Edge/Fast Tasks:** `Llama 3.2 3B`.
    * Utilizado apenas para tarefas de sumariza√ß√£o simples onde a baixa lat√™ncia √© priorit√°ria.

    ### ‚ö†Ô∏è Limita√ß√µes de Racioc√≠nio (Known Issues)

Testes de l√≥gica temporal demonstraram que modelos da classe 8B (Llama 3.1) tendem a falhar em "pegadinhas" sem√¢nticas simples (Zero-shot prompting).
* **Mitiga√ß√£o:** A implementa√ß√£o de t√©cnicas de *Chain-of-Thought (CoT)* e *Few-Shot Prompting* mostrou-se necess√°ria para for√ßar o modelo a decompor o problema l√≥gico antes da gera√ß√£o da resposta final.

## üé® Pipeline de Vis√£o Computacional (Stable Diffusion)

Para a gera√ß√£o de imagens, implementei uma arquitetura baseada em n√≥s (Node-based Architecture) utilizando o **ComfyUI**.

### Decis√µes de Arquitetura:
* **Workflow Visual:** Diferente de interfaces monol√≠ticas, o ComfyUI permite a visualiza√ß√£o e manipula√ß√£o granular do fluxo de tensores latentes (Latent Space tensors).
* **Otimiza√ß√£o de VRAM:** O ComfyUI gerencia a mem√≥ria da GPU de forma agressiva, carregando e descarregando modelos do VRAM conforme necess√°rio, permitindo a execu√ß√£o de workflows complexos (ex: High-Res Fix, ControlNet) mesmo com limita√ß√µes de hardware.

### Stack Tecnol√≥gica:
* **Framework:** PyTorch (com suporte a CUDA 12.x).
* **Motor de Difus√£o:** Latent Diffusion Models (LDMs).
* **Ambiente:** Execu√ß√£o isolada via `python venv` no WSL 2 para garantir a reprodutibilidade das depend√™ncias.

### üì¶ Gest√£o de Modelos e Extens√µes

Para mitigar problemas de "Link Rot" (URLs quebradas) em scripts de automa√ß√£o, implementei duas estrat√©gias de redund√¢ncia:
1.  **Fallback Repositories:** Utiliza√ß√£o do Hugging Face Hub como fonte prim√°ria para checkpoints pesados (safetensors), devido √† maior estabilidade de banda e imutabilidade dos links em compara√ß√£o ao Civitai.
2.  **ComfyUI Manager:** Instala√ß√£o do orquestrador de depend√™ncias `ltdrdata/ComfyUI-Manager`. Isso permite a instala√ß√£o via GUI de Custom Nodes e modelos, resolvendo automaticamente conflitos de depend√™ncia Python e atualiza√ß√µes de vers√£o.

## ‚úÖ Conclus√£o e Pr√≥ximos Passos

O projeto foi conclu√≠do com sucesso, estabelecendo um laborat√≥rio de IA generativa local e privado, capaz de executar tarefas de processamento de linguagem natural e vis√£o computacional sem depend√™ncia de nuvem.

**Resultados Chave:**
* **Infraestrutura H√≠brida:** Sucesso na integra√ß√£o WSL 2 + Docker + NVIDIA Container Toolkit, provando ser uma arquitetura vi√°vel e perform√°tica para desenvolvimento de IA no Windows.
* **LLM (Ollama):** Implementa√ß√£o de modelos capazes de auxiliar em tarefas de programa√ß√£o (C/Python) com lat√™ncia zero de rede.
* **Image Gen (ComfyUI):** Gera√ß√£o de imagens fotorrealistas (SDXL) em menos de 30 segundos [tempo estimado], demonstrando o poder de processamento da RTX 5060 para cargas de trabalho criativas.

**Roadmap Futuro (F√©rias):**
* Explorar **ControlNet** para guiar a gera√ß√£o de imagens usando esbo√ßos ou poses.
* Testar **Upscaling com IA** para transformar as imagens de 1024px em 4K.
* Criar um bot de Telegram que se conecta ao meu Ollama local para conversar pelo celular.