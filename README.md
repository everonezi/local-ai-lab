# üß™ Local AI & High-Performance Computing Lab

> *Implementa√ß√£o de um ambiente local de Intelig√™ncia Artificial Generativa e Computa√ß√£o de Alto Desempenho utilizando hardware de consumo (RTX 5060).*

![Exemplo de Gera√ß√£o Cyberpunk](result-cyberpunk.png)
*Exemplo de gera√ß√£o: Prompt "Cyberpunk Arch Linux Setup" renderizado via Juggernaut XL em ~15s.*

---

## üìñ Sobre o Projeto
Este reposit√≥rio documenta a cria√ß√£o de um laborat√≥rio de IA "Full-Stack" rodando localmente. O objetivo foi eliminar a depend√™ncia de APIs em nuvem (OpenAI/Google), garantindo privacidade total de dados e lat√™ncia zero, al√©m de explorar a arquitetura de sistemas Linux dentro do ecossistema Windows via virtualiza√ß√£o leve.

---

## üõ†Ô∏è Infraestrutura e Arquitetura

### Sistema Operacional: H√≠brido (Windows 11 + WSL 2)
A infraestrutura baseia-se no **WSL 2 (Windows Subsystem for Linux)**.

* **Decis√£o T√©cnica:**
    * Permite o uso de ferramentas nativas Linux (Ubuntu 22.04 LTS) essenciais para Data Science.
    * **GPU Passthrough:** O WSL 2 oferece acesso direto aos drivers da NVIDIA (CUDA) instalados no Windows, permitindo que o kernel Linux utilize todo o poder da RTX 5060 para c√°lculos tensoriais sem a sobrecarga de uma VM tradicional.
    * **Docker Integration:** Elimina problemas de compatibilidade de *filesystem* comuns no Docker Desktop for Windows.

**Specs do Hardware:**
* **CPU:** Intel Core i5 12400F (Single-core performance para emula√ß√£o/compila√ß√£o).
* **GPU:** NVIDIA RTX 5060 8GB (Tensor Cores para IA).
* **RAM:** 32GB (Essencial para carregar modelos LLM na mem√≥ria).

### ‚öôÔ∏è Configura√ß√£o do Ambiente
1.  **Linux Distro:** Ubuntu 22.04 LTS (Jammy Jellyfish).
2.  **Permiss√µes:** Configura√ß√£o de usu√°rio n√£o-root com privil√©gios `sudo` e inclus√£o no grupo `docker`.
3.  **Drivers:** Utiliza√ß√£o do *NVIDIA Container Toolkit* para permitir que containers Docker acessem a GPU atrav√©s da camada de abstra√ß√£o do DirectX 12/WDDM 2.9.

---

## üß† O "C√©rebro": LLMs e Infer√™ncia de Texto

Para a execu√ß√£o de modelos de linguagem, utilizei uma arquitetura containerizada.

### Backend: Ollama
Servidor de infer√™ncia local otimizado para chips Apple Silicon e NVIDIA.
* **Performance:** A execu√ß√£o ocorre via chamadas CUDA diretas.
* **Modelos Selecionados (Model Zoo):**
    * **Llama 3.1 8B (Quantizado):** Equil√≠brio entre racioc√≠nio l√≥gico e velocidade.
    * **DeepSeek Coder V2:** Modelo especializado (Fine-tuned) em programa√ß√£o, superando modelos generalistas em tarefas de C e Java.

### Frontend: Open WebUI
Interface visual moderna rodando em Docker, conectada ao Ollama via rede interna.
* **Stack:** Docker Container na porta 3000.
* **Features:** Hist√≥rico persistente, Syntax Highlighting para c√≥digo e suporte a RAG (Retrieval-Augmented Generation).

![Interface do Open WebUI](webui-chat.png)

---

## üé® A "Vis√£o": Pipeline de Gera√ß√£o de Imagens

Implementa√ß√£o de um pipeline de *Stable Diffusion XL* baseado em n√≥s (Nodes).

### Engine: ComfyUI
Diferente de interfaces simples, o **ComfyUI** permite a manipula√ß√£o granular do fluxo de tensores latentes.
* **Workflow:** Carregamento de Checkpoint -> Prompt Positivo/Negativo -> KSampler -> VAE Decode.
* **Gerenciamento de VRAM:** Otimiza√ß√£o agressiva para rodar modelos SDXL (6GB+) em uma placa de 8GB, utilizando *offloading* inteligente para a RAM do sistema.

![Workflow de N√≥s no ComfyUI](comfyui-workflow.png)

### Ferramentas de MLOps:
* **ComfyUI Manager:** Orquestrador para instala√ß√£o autom√°tica de depend√™ncias e Custom Nodes.
* **Fallback Strategy:** Uso de scripts `curl` com redund√¢ncia (Hugging Face) para baixar modelos quando reposit√≥rios prim√°rios (Civitai) apresentam instabilidade.

---

## üîß Desafios e Troubleshooting (Log de Engenharia)

Durante a implementa√ß√£o, documentei e solucionei os seguintes problemas t√©cnicos:

### 1. Networking em Containers
* **Problema:** O container do WebUI n√£o encontrava o Ollama.
* **Diagn√≥stico:** O Ollama escuta em `127.0.0.1` (localhost) por padr√£o, rejeitando conex√µes externas do Docker.
* **Solu√ß√£o:** Configurei a vari√°vel `OLLAMA_HOST=0.0.0.0` via systemd e utilizei o DNS interno `host.docker.internal` para ponte de rede.

### 2. Deprecia√ß√£o de Bibliotecas Linux
* **Problema:** Erro ao iniciar o ComfyUI: `ImportError: libGL.so.1`.
* **Causa:** O pacote `libgl1-mesa-glx` foi descontinuado no Ubuntu 22.04+.
* **Solu√ß√£o:** Migra√ß√£o para o pacote moderno `libgl1` (Vendor-Neutral GL dispatch library).

### 3. "Cold Start" da Aplica√ß√£o
* **Sintoma:** Erro `ERR_EMPTY_RESPONSE` no navegador nos primeiros segundos.
* **Li√ß√£o:** Necessidade de aguardar o *boot* completo do servidor Uvicorn interno antes de tentar conex√£o HTTP.

---

## ‚ö° Guia de Execu√ß√£o (Runbook)

Como iniciar o laborat√≥rio ap√≥s reiniciar a m√°quina:

1.  **Acordar a Infraestrutura:**
    ```bash
    sudo service docker start   # Inicia o Docker
    sudo systemctl start ollama # Inicia o Backend de IA
    ```
2.  **Subir o Laborat√≥rio de Imagem:**
    ```bash
    cd ~/ComfyUI
    source venv/bin/activate    # Ativar ambiente Python isolado
    python main.py              # Iniciar servidor na porta 8188
    ```
3.  **Acessos:**
    * **Chat & Code:** `http://localhost:3000`
    * **Image Studio:** `http://localhost:8188`

---

## ‚úÖ Conclus√£o

Este projeto validou a viabilidade de desenvolvimento de IA de alta performance em ambiente Windows dom√©stico. A combina√ß√£o de **WSL 2 + Docker + CUDA** provou-se uma stack robusta, permitindo iterar projetos de faculdade e prot√≥tipos de software com privacidade e sem custos de nuvem.